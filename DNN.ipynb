{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DNN.ipynb","provenance":[{"file_id":"1deH4qkjwsLdSMOLCqhuCmucNFlfFK0pa","timestamp":1568668621133},{"file_id":"1ds60dJaWmwAC9KzEhWIOy0JtaH60lKnm","timestamp":1568319409574}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fSIfBsgi8dNK","colab_type":"code","colab":{}},"source":["#@title Authored by Michael Evans. { display-mode: \"form\" }\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AC8adBmw-5m3","colab_type":"text"},"source":["# Introduction\n","\n","This is an Earth Engine <> TensorFlow notebook demonstrating workflows to train an automated land cover change prediction DNN model.  Specifically, this notebook shows:\n","\n","1.   Ingesting previously exported csv files into TFRecord format.\n","2.   Preparing the data for use in a TensorFlow model.\n","2.   Training and validating a simple model (Keras `Sequential` neural network) in TensorFlow.\n","3.   Making predictions on image data exported from Earth Engine in TFRecord format.\n","4.   Ingesting classified image data to Earth Engine in TFRecord format."]},{"cell_type":"markdown","metadata":{"id":"tVu4g8V5yYC5","colab_type":"text"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"KiTyR3FNlv-O","colab_type":"text"},"source":["## Install the Earth Engine client library\n","\n","This only needs to be done once per notebook."]},{"cell_type":"code","metadata":{"id":"sYyTIPLsvMWl","colab_type":"code","cellView":"code","colab":{}},"source":["!pip install earthengine-api"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7W-fPY-ox93Y","colab_type":"text"},"source":["Authenticate to Earth Engine"]},{"cell_type":"code","metadata":{"id":"NnAX6VAbyAFH","colab_type":"code","colab":{}},"source":["import ee\n","ee.Authenticate()\n","ee.Initialize()\n","# Test the earthengine command by getting help on upload.\n","!earthengine upload image -h"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qptAXhKmXo_J","colab_type":"text"},"source":["## Google Authentication\n","\n","To read/write from a Google Cloud Storage bucket to which you have access, it's necessary to authenticate (as yourself).  You'll also need to authenticate as yourself with Earth Engine, so that you'll have access to your scripts, assets, etc."]},{"cell_type":"code","metadata":{"id":"5qMKG1hEXuML","colab_type":"code","cellView":"code","colab":{}},"source":["from google.colab import auth, drive\n","\n","auth.authenticate_user()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itbldLRryfR0","colab_type":"text"},"source":["Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"KGr2dyod1b98","colab_type":"code","colab":{}},"source":["ROOT = ('/content/drive')\n","drive.mount(ROOT)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aO9GIpiYDCS","colab_type":"text"},"source":["Make sure python can reference our module library"]},{"cell_type":"code","metadata":{"id":"eHYuX-yRYHBD","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/repos/ACD_methods/EEcode/Python')\n","sys.path"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMkNljPGMzyG","colab_type":"text"},"source":["## Set up Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"iJ70EsoWND_0","colab_type":"text"},"source":["The default public runtime already has the tensorflow libraries we need installed.  Before any operations from the TensorFlow API are used, import TensorFlow. Eager execution should be enabled by default as of TF v2.x[`tf.enable_eager_execution()` docs](https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution). "]},{"cell_type":"code","metadata":{"id":"i1PrYRLaVw_g","colab_type":"code","cellView":"code","colab":{}},"source":["import tensorflow as tf\n","\n","tf.executing_eagerly()\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8Xcvjp6cLOL","colab_type":"text"},"source":["## Set up Folium\n","\n","The default public runtime already has the Folium library we will use for visualization.  Import the library, check the version, and define the URL where Folium will look for Earth Engine generated map tiles."]},{"cell_type":"code","metadata":{"id":"YiVgOXzBZJSn","colab_type":"code","colab":{}},"source":["import folium\n","print(folium.__version__)\n","\n","# Define a method for displaying Earth Engine image tiles to a folium map.\n","def add_ee_layer(self, ee_image_object, vis_params, name):\n","  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n","  folium.raster_layers.TileLayer(\n","    tiles = map_id_dict['tile_fetcher'].url_format,\n","    attr = \"Map Data Â© Google Earth Engine\",\n","    name = name,\n","    overlay = True,\n","    control = True\n","  ).add_to(self)\n","\n","# Add EE drawing method to folium.\n","folium.Map.add_ee_layer = add_ee_layer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcjQnHH8zT4q","colab_type":"text"},"source":["# Get Training and Testing data from Earth Engine\n","\n","We have previously exported csv files to Google Drive that contain per-pixel output from our change detection algorithms. This output consists of 6 metrics, and a field indicating whether that pixel is a true 'changed' pixel or not."]},{"cell_type":"code","metadata":{"id":"3Dyj4tCW22T-","colab_type":"code","colab":{}},"source":["from os.path import join\n","DATA_DIR = 'My Drive/EarthEngine/ACD/IW/S2/S2_2'\n","# Number of records in dataset\n","SIZE = 4826575\n","BATCH = 10\n","NUMERIC_COLUMNS = ['cv_z', 'ndvi_z', 'nbr_z', 'ndwi_z', 'rcvmax_z', 'ndsi_z']\n","CATEGORY_COLUMNS = {'habitat':['shrub', 'forest', 'desert', 'grassland', 'wetland']}\n","\n","FEATURE_COLUMNS = NUMERIC_COLUMNS + list(CATEGORY_COLUMNS.keys())\n","LABEL_COLUMN = 'disturbance'\n","LABELS = ['none', 'bare', 'residential', 'solar']\n","# LABEL_COLUMN = 'change'\n","# LABELS = [0, 1]\n","# factor for train/test split\n","FACTOR = 5\n","BUCKET = 'cvod-203614-mlengine'\n","PROJECT = 'ACD_methods'\n","PREDICT_DIR = 'data/predict'\n","LOG_DIR = 'drive/My Drive/Tensorflow/models/ACD_DNN'\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiMBKqBS1Y6k","colab_type":"code","colab":{}},"source":["def get_csv_dataset(file_path, batch, labels, features, **kwargs):\n","  \"\"\"\n","  Construct a tfrecord dataset from a list of csv files\n","  Parameters:\n","    file_path (list<str>): string or list of strings specifying input files\n","    batch (int): batch size\n","    labels (str): field name containing labels\n","    features(list<str>): field name(s) containing feature values\n","  Returns:\n","    tf.data.Dataset\n","  \"\"\"\n","  dataset = tf.data.experimental.make_csv_dataset(\n","      file_path,\n","      batch_size= batch,\n","      label_name= labels,\n","      select_columns = features + [labels],\n","      na_value=\"?\",\n","      # one epoch of data initially because otherwise splitting runs infinitely\n","      num_epochs=1,\n","      ignore_errors=True,\n","      **kwargs)\n","  return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6H2rreBQzwFy","colab_type":"code","colab":{}},"source":["# Specify the path to our csv data\n","csv_file_path = join(ROOT, DATA_DIR)\n","\n","# Put all the csv files from our data directory into a list\n","rawData = tf.io.gfile.glob(csv_file_path + '/*.csv')\n","\n","# Create a TFDataset\n","tfData = get_csv_dataset(rawData, BATCH, LABEL_COLUMN, FEATURE_COLUMNS)\n","\n","# Inspect a batch of records\n","# # take creates a new dataset with n elements (batches)\n","# test = tfData.take(1)\n","# feats, labs = iter(test).next()\n","# # features are a dictionary of tensors\n","# print(feats)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MX7wwcU4RNit","colab_type":"code","colab":{}},"source":["def preprocess_record(features, labels):\n","  \"\"\"\n","  Process input data by converting categorical features to lowercase and labels to one-hot\n","  Parameters:\n","    features (list<str>): column names of features to be used for prediction\n","    labels (str): column name containing labels\n","  Returns:\n","    tuple: dictionary of features and label tensor\n","  \"\"\"\n","  # convert labels to one_hot tensor\n","  if labels.dtype.__eq__(tf.dtypes.string):\n","    # manually convert strings to one-hot tensor\n","    matches = tf.stack([tf.equal(labels, s) for s in LABELS], axis = -1)\n","    labels = tf.cast(matches, tf.float32)\n","  else:\n","    labels = tf.one_hot(labels, len(LABELS))\n","  # change all strings to lowercase\n","  features = {key:tf.strings.lower(feature) if key in CATEGORY_COLUMNS.keys() else feature for key, feature in features.items()}\n","  return features, labels\n","\n","def train_test_split(dataset, factor):\n","  \"\"\"\n","  Divide a tf.data.Dataset into train and test splits\n","  Parameters:\n","    dataset (tf.data.Dataset): dataset with features and labels to split\n","    factor (int): numerator for fraction of testing data (e.g. 5 = 1/5)\n","  Returns:\n","    tuple: two tf.data.Datasets\n","  \"\"\"\n","  def is_test(x, y):\n","    return x % factor == 0\n","\n","  def is_train(x, y):\n","    return not is_test(x, y)\n","\n","  def recover(x,y):\n","    return y\n","\n","  dataset = dataset.enumerate()\n","  test = dataset.filter(is_test).map(recover)\n","  train = dataset.filter(is_train).map(recover)\n","  return test, train\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAR-Oi17HHf_","colab_type":"code","colab":{}},"source":["# process the data and shuffle once before splitting\n","tfData = tfData.map(preprocess_record).shuffle(SIZE, reshuffle_each_iteration = False)\n","\n","# split into testing and training data\n","testData, trainData = train_test_split(tfData, FACTOR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3drz8O44lf6","colab_type":"code","colab":{}},"source":["# Inspect our splits. These should be shuffled and batched\n","# take creates a new dataset with n elements (batches)\n","test = testData.take(1)\n","feats, labs = iter(test).next()\n","# features are a dictionary of tensors\n","print(labs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQW46jQ4aFww","colab_type":"code","colab":{}},"source":["print(feats['cv_z'].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9pWa54oG-xl","colab_type":"text"},"source":["# Create the Keras model\n","\n","Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss.  Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See [the Keras loss function docs](https://keras.io/losses/), [the TensorFlow categorical identity docs](https://www.tensorflow.org/guide/feature_columns#categorical_identity_column) and [the `tf.one_hot` docs](https://www.tensorflow.org/api_docs/python/tf/one_hot) for details).  \n","\n","Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer.  Once the dataset has been prepared, define the model, compile it, fit it to the training data.  See [the Keras `Sequential` model guide](https://keras.io/getting-started/sequential-model-guide/) for more details."]},{"cell_type":"code","metadata":{"id":"ujZo_8qEZVFw","colab_type":"code","colab":{}},"source":["# Create feature columns for feature data as part of graph\n","numericColumns = [tf.feature_column.numeric_column(ft) for ft in NUMERIC_COLUMNS]\n","categoricalColumns = [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(key, vocab)) for key, vocab in CATEGORY_COLUMNS.items() if key == 'habitat']\n","preprocessing_layer = tf.keras.layers.DenseFeatures(categoricalColumns + numericColumns)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH6QOMtI3ECk","colab_type":"code","colab":{}},"source":["print(preprocessing_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCZq3VNpG--G","colab_type":"code","cellView":"code","colab":{}},"source":["from tensorflow import keras\n","\n","# Define the layers in the model.\n","model = tf.keras.models.Sequential([\n","  preprocessing_layer,                                 \n","  tf.keras.layers.Dense(64, input_shape = (7,), activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(len(LABELS), activation=tf.nn.softmax)\n","])\n","\n","# Define metrics we want to monitor\n","fp = tf.keras.metrics.FalsePositives()\n","fn = tf.keras.metrics.FalseNegatives()\n","\n","tensorboard = tf.keras.callbacks.TensorBoard(log_dir = LOG_DIR)\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    join(LOG_DIR,'best_weights.hdf5'),\n","    monitor='val_accuracy',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max'\n","    )\n","\n","# Compile the model with the specified loss function.\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy', fp, fn])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7O0oFZl3NUkR","colab_type":"code","colab":{}},"source":["# Fit the model to the training data.\n","# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n","model.fit(\n","    # need to repeat our training data to cover multiple epochs\n","    x = trainData.repeat(),\n","    epochs = 5,\n","    steps_per_epoch = (SIZE*(FACTOR-1)/FACTOR)/BATCH,\n","    validation_data = testData,\n","    validation_steps = (SIZE/FACTOR)/BATCH,\n","    callbacks = [checkpoint, tensorboard]\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSWG__O9Hg0C","colab_type":"code","colab":{}},"source":["# Fit the model to initialize weights so we can restore from checkpoint\n","model.fit(\n","    # need to repeat our training data to cover multiple epochs\n","    x = trainData.repeat(),\n","    epochs = 1,\n","    steps_per_epoch = 10\n","    )\n","\n","model.save(join(LOG_DIR, 'DNN_64.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmAxyRkMKOPR","colab_type":"code","colab":{}},"source":["#bring in the architecture and best weights from Drive\n","model = tf.keras.models.load_model(join(LOG_DIR, 'DNN_64.h5'))\n","# load pre-trained weights from best performing epoch\n","model.load_weights(join(LOG_DIR, 'best_weights.hdf5')) \n","\n","#lets see where were at\n","# evalMetrics = model.evaluate(x=testData, steps = (SIZE/FACTOR)/BATCH, verbose = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pa4ex_4eKiyb","colab_type":"text"},"source":["## Check model accuracy on the test set\n","\n","Now that we have a trained model, we can evaluate it using the test dataset.  To do that, read and prepare the test dataset in the same way as the training dataset.  Here we specify a batch sie of 1 so that each example in the test set is used exactly once to compute model accuracy.  For model steps, just specify a number larger than the test dataset size (ignore the warning)."]},{"cell_type":"markdown","metadata":{"id":"jO16JbuIHWOo","colab_type":"text"},"source":["# Prediction"]},{"cell_type":"markdown","metadata":{"id":"0sWUnbNRHfWt","colab_type":"text"},"source":["Now that we have a trained model, let's make some predictions on images from GEE. First, we need to export an image to make predictions on"]},{"cell_type":"code","metadata":{"id":"KwKZwjNbHivx","colab_type":"code","colab":{}},"source":["# Run the IW algorithm to generate an image with change metrics\n","import analyze, dictionaries\n","\n","nlcd = ee.Image('USGS/NLCD/NLCD2016')\n","\n","# give this aoi a name\n","testId = 'HughesMillCA'\n","\n","# TODO: split up analyze functions so we don't need these\n","# grab the relevant dictionary of lda coefficients\n","dictionary = dictionaries.forest\n","\n","aoi = ee.Geometry.Polygon(\n","        [[[-120.83127262496578, 39.10457008576222],\n","          [-120.83127262496578, 39.06952752960459],\n","          [-120.76518299483882, 39.06952752960459],\n","          [-120.76518299483882, 39.10457008576222]]], None, False);\n","\n","doi = '2017-07-01' \n","\n","landcover = 'forest'\n","\n","output = analyze.analyze_iw(\n","    ee.Feature(aoi, {'mode':landcover}),\n","     doi, dictionary, 0, testId)\n","iwImg = output[4]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xi81yzlsNo8","colab_type":"code","colab":{}},"source":["# check the iwoutput on map\n","map = folium.Map(location=[39.08, -120.80])\n","map.add_ee_layer(iwImg, {'bands':['cv_z'], 'min':0, 'max': 50}, 'iwout')\n","map"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NA8QA8oQVo8V","colab_type":"text"},"source":["## Export the imagery\n","\n","You can also export imagery using TFRecord format.  Specifically, export whatever imagery you want to be classified by the trained model into the output Cloud Storage bucket."]},{"cell_type":"code","metadata":{"id":"L4NV6oIaHeQf","colab_type":"code","colab":{}},"source":["def doExport(image, out_image_base, directory, region):\n","  \"\"\"\n","  Export a GEE image as TFRecord.  Block until complete.\n","  Parameters:\n","    image (ee.Image): image to be exported\n","    out_image_base (str): output image base filename\n","    directory (str): google cloud directory for image export\n","    region (ee.Geometry): bounding area\n","  \"\"\"\n","  # Specify patch and file dimensions.\n","  imageExportFormatOptions = {\n","    'patchDimensions': [256, 256],\n","    'maxFileSize': 104857600,\n","    'compressed': True\n","  }\n","\n","  task = ee.batch.Export.image.toCloudStorage(\n","    image = image,\n","    description = out_image_base,\n","    fileNamePrefix = join(directory, out_image_base),\n","    bucket = BUCKET,\n","    scale = 10,\n","    fileFormat = 'TFRecord',\n","    region = region,\n","    formatOptions = imageExportFormatOptions,\n","  )\n","  task.start()\n","\n","  # Block until the task completes.\n","  print('Running image export to Cloud Storage...')\n","  import time\n","  while task.active():\n","    time.sleep(30)\n","\n","  # Error condition\n","  if task.status()['state'] != 'COMPLETED':\n","    print('Error with image export.')\n","  else:\n","    print('Image export completed.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SweCkHDaNE3","colab_type":"code","cellView":"both","colab":{}},"source":["# Start the task.\n","doExport(iwImg, testId, join(PROJECT, PREDICT_DIR), aoi)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nhHrnv3VR0DU","colab_type":"text"},"source":["## Use the trained model to classify an image from Earth Engine\n","\n","Now it's time to classify the image that was exported from Earth Engine.  If the exported image is large, it will be split into multiple TFRecord files in its destination folder.  There will also be a JSON sidecar file called \"the mixer\" that describes the format and georeferencing of the image.  Here we will find the image files and the mixer file, getting some info out of the mixer that will be useful during model inference."]},{"cell_type":"markdown","metadata":{"id":"nmTayDitZgQ5","colab_type":"text"},"source":["Use `gsutil` to locate the files of interest in the output Cloud Storage bucket.  Check to make sure your image export task finished before running the following."]},{"cell_type":"code","metadata":{"id":"oUv9WMpcVp8E","colab_type":"code","colab":{}},"source":["# Get a list of all the files in the output bucket.\n","def get_pred_files(imageBase, inDir):\n","  \"\"\"\n","  Retrieve TFRecord image files and json mixer exported from GEE\n","  Parameters:\n","    imageBase (str): base filename for images to return\n","    inDir (str): directory containing image and mixer files\n","  Returns:\n","    tuple: list of image filenames, json\n","  \"\"\"\n","  \n","  filesList = !gsutil ls {inDir}\n","  print(filesList)\n","  # Get only the files generated by the image export.\n","  exportFilesList = [s for s in filesList if imageBase in s]\n","\n","  # Get the list of image files and the JSON mixer file.\n","  imageFilesList = []\n","  jsonFile = None\n","  for f in exportFilesList:\n","    if f.endswith('.tfrecord.gz'):\n","      imageFilesList.append(f)\n","    elif f.endswith('.json'):\n","      jsonFile = f\n","\n","  # Make sure the files are in the right order.\n","  imageFilesList.sort()\n","\n","  # pprint(imageFilesList)\n","  # print(jsonFile)\n","\n","  import json\n","  # Load the contents of the mixer file to a JSON object.\n","  jsonText = !gsutil cat {jsonFile}\n","  # Get a single string w/ newlines from the IPython.utils.text.SList\n","  mixer = json.loads(jsonText.nlstr)\n","  print(mixer)\n","  return imageFilesList, mixer\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6xyzyPPJwpVI","colab_type":"text"},"source":["## Read the image files into a dataset\n","\n","You can feed the list of files (`imageFilesList`) directly to the `TFRecordDataset` constructor to make a combined dataset on which to perform inference.  The input needs to be preprocessed differently than the training and testing.  Mainly, this is because the pixels are written into records as patches, we need to read the patches in as one big tensor (one patch for each band), then flatten them into lots of little tensors."]},{"cell_type":"code","metadata":{"id":"EaX_b6dJ7oJR","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tn8Kj3VfwpiJ","colab_type":"code","cellView":"code","colab":{}},"source":["# Get relevant info from the JSON mixer file.\n","def make_pred_dataset(imageBase, inDir, features, habType):\n","  \"\"\"\n","  This needs to return a tuple <dictionary of prediction features, blank 'labels'>\n","  \"\"\"\n","  imageFiles, mixer = get_pred_files(imageBase, inDir)\n","  patch_width = mixer['patchDimensions'][0]\n","  patch_height = mixer['patchDimensions'][1]\n","  patches = mixer['totalPatches']\n","  patch_dimensions_flat = [patch_width * patch_height, 1]\n","\n","  # get the index of the habitat column\n","  featList = features\n","  hab = featList.index('habitat')\n","  hab = featList.pop(hab)\n","  print(hab)\n","  # Note that the tensors are in the shape of a patch, one patch for each band.\n","  imageColumns = [\n","    tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) \n","      for k in featList\n","  ]\n","\n","  # Parsing dictionary.\n","  imageFeaturesDict = dict(zip(featList, imageColumns))\n","\n","  # Note that you can make one dataset from many files by specifying a list.\n","  imageDataset = tf.data.TFRecordDataset(imageFiles, compression_type='GZIP')\n","\n","  # Parsing function.\n","  # Each element in the output is a dictionary of fixedlenfeatures of size (65536, 1)\n","  # There will be as many elements as patches\n","  def parse_image(example_proto):\n","    parsed = tf.io.parse_single_example(example_proto, imageFeaturesDict)\n","    # add habitat tensor to dictionary\n","    parsed.update({'habitat':np.full(patch_dimensions_flat, habType)})\n","    return parsed\n","\n","  # Parse the data into tensors, one long tensor per patch.\n","  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n","\n","  # Break our long tensors into many little ones. Only necessary if we want to do calculations with the data(?)\n","  # creates tensors for each feature of shape (1, )\n","  imageDataset = imageDataset.flat_map(\n","  # slice each tensor along first dimension\n","    lambda x: tf.data.Dataset.from_tensor_slices(x)\n","  )\n","\n","  # # Add additional features (NDVI).\n","  # # imageDataset = imageDataset.map(\n","  # #   # Add NDVI to a feature that doesn't have a label.\n","  # #   lambda features: addNDVI(features, None)[0]\n","  # # )\n","\n","  # Turn the dictionary in each record into a tuple with a dummy label.\n","  # imageDataset = imageDataset.map(\n","  #   # The model expects a tuple of (dictionary, label).\n","  #   # lambda dataDict: (dataDict, )\n","  #   # add dimension with 'list' then transpose from (6, 1) to (1, 6)\n","  #   # this operation destroys the dictionary\n","  #   lambda dataDict: (tf.transpose(list(dataDict.values())), )\n","  # )\n","\n","  # Turn each patch into a batch.\n","  # This creates element tensors of shape (65536, 1, 6)\n","  imageDataset = imageDataset.batch(patch_width * patch_height)\n","  # imageDataset = imageDataset.batch(1)\n","  return imageDataset, patches"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2sfRemRRDkV","colab_type":"text"},"source":["## Generate predictions for the image pixels\n","\n","To get predictions in each pixel, run the image dataset through the trained model using `model.predict()`.  Print the first prediction to see that the output is a list of the three class probabilities for each pixel.  Running all predictions might take a while."]},{"cell_type":"code","metadata":{"id":"6mKO94wcyQxz","colab_type":"code","colab":{}},"source":["FEATURE_COLUMNS.append('habitat')\n","FEATURE_COLUMNS"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VGhmiP_REBP","colab_type":"code","colab":{}},"source":["# Run prediction in batches, with as many steps as there are patches.\n","def make_predictions(imgBase, inDir, model, features, habType):\n","  imageDataset, patches = make_pred_dataset(imgBase, inDir, features, habType) \n","  predictions = model.predict(imageDataset, steps = patches, verbose = 1)\n","  return predictions\n","\n","# Note that the predictions come as a numpy array.  Check the first one.\n","predImgDir = join('gs://', BUCKET, PROJECT, PREDICT_DIR)\n","predictions = make_predictions(testId, predImgDir, model, FEATURE_COLUMNS, 'forest')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPU2VlPOikAy","colab_type":"text"},"source":["## Write the predictions to a TFRecord file\n","\n","Now that there's a list of class probabilities in `predictions`, it's time to write them back into a file, optionally including a class label which is simply the index of the maximum probability.  We'll write directly from TensorFlow to a file in the output Cloud Storage bucket.\n","\n","Iterate over the list, compute class label and write the class and the probabilities in patches.  Specifically, we need to write the pixels into the file as patches in the same order they came out.  The records are written as serialized `tf.train.Example` protos.  This might take a while."]},{"cell_type":"code","metadata":{"id":"AkorbsEHepzJ","colab_type":"code","colab":{}},"source":["outputImageFile = join('gs://', BUCKET, PROJECT, PREDICT_DIR, 'output', testId + '.TFRecord')\n","outputImageFile\n","PATCH_WIDTH = 256\n","PATCH_HEIGHT = 256\n","PATCHES = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kATMknHc0qeR","colab_type":"code","cellView":"both","colab":{}},"source":["# Don't run\n","# Instantiate the writer.\n","writer = tf.io.TFRecordWriter(outputImageFile)\n","\n","# Every patch-worth of predictions we'll dump an example into the output\n","# file with a single feature that holds our predictions. Since our predictions\n","# are already in the order of the exported data, the patches we create here\n","# will also be in the right order.\n","patch = [[], [], [], [], []]\n","curPatch = 1\n","for prediction in predictions:\n","  patch[0].append(tf.argmax(prediction, 0))\n","  patch[1].append(prediction[0])\n","  patch[2].append(prediction[1])\n","  patch[3].append(prediction[2])\n","  patch[4].append(prediction[3])\n","  # Once we've seen a patches-worth of class_ids...\n","  if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):\n","    print('Done with patch ' + str(curPatch) + ' of ' + str(PATCHES) + '...')\n","    # Create an example\n","    # TODO: use dict comprehension to make this generalizeable based on labels\n","    example = tf.train.Example(\n","      features=tf.train.Features(\n","        feature={\n","          'prediction': tf.train.Feature(\n","              int64_list=tf.train.Int64List(\n","                  value=patch[0])),\n","          'noneProb': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=patch[1])),\n","          'bareProb': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=patch[2])),\n","          'resProb': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=patch[3])),\n","          'solarProb': tf.train.Feature(\n","              float_list=tf.train.FloatList(\n","                  value=patch[4]))\n","        }\n","      )\n","    )\n","    # Write the example to the file and clear our patch array so it's ready for\n","    # another batch of class ids\n","    writer.write(example.SerializeToString())\n","    patch = [[], [], [], [], []]\n","    curPatch += 1\n","\n","writer.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1K_1hKs0aBdA","colab_type":"text"},"source":["# Upload the classifications to an Earth Engine asset"]},{"cell_type":"markdown","metadata":{"id":"M6sNZXWOSa82","colab_type":"text"},"source":["## Verify the existence of the predictions file\n","\n","At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket.  Use the `gsutil` command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size."]},{"cell_type":"code","metadata":{"id":"6ZVWDPefUCgA","colab_type":"code","colab":{}},"source":["!gsutil ls -l {outputImageFile}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZyCo297Clcx","colab_type":"text"},"source":["## Upload the classified image to Earth Engine\n","\n","Upload the image to Earth Engine directly from the Cloud Storage bucket with the [`earthengine` command](https://developers.google.com/earth-engine/command_line#upload).  Provide both the image TFRecord file and the JSON file as arguments to `earthengine upload`.  (You can use the `nclinton` version for now.)"]},{"cell_type":"code","metadata":{"id":"l0HpcR3FM3yw","colab_type":"code","colab":{}},"source":["!gsutil ls gs://cvod-203614-mlengine/ACD_methods/data/predict/*.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXulMNl9lTDv","colab_type":"code","cellView":"code","colab":{}},"source":["USER_NAME = 'defendersofwildlifeGIS'\n","outputAssetID = 'users/' + USER_NAME + '/' + testId\n","jsonFile = 'gs://cvod-203614-mlengine/ACD_methods/data/predict/HughesMillCA-mixer.json'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V64tcVxsO5h6","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Don't run\n","\n","# Start the upload.\n","\n","!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {jsonFile}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yt4HyhUU_Bal","colab_type":"text"},"source":["## Check the status of the asset ingestion\n","\n","You can also use the Earth Engine API to check the status of your asset upload.  It might take a while.  The upload of the image is an asset ingestion task.  "]},{"cell_type":"code","metadata":{"id":"_vB-gwGhl_3C","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Don't run\n","ee.batch.Task.list()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvXvy9GDhM-p","colab_type":"text"},"source":["## View the ingested asset\n","\n","Display the vector of class probabilities as an RGB image with colors corresponding to the probability of bare, vegetation, water in a pixel.  Also display the winning class using the same color palette."]},{"cell_type":"code","metadata":{"id":"kEkVxIyJiFd4","colab_type":"code","colab":{}},"source":["predictionsImage = ee.Image(outputAssetID)\n","\n","predictionVis = {\n","  'bands': 'prediction',\n","  'min': 0,\n","  'max': 2,\n","  'palette': ['red', 'green', 'blue']\n","}\n","probabilityVis = {'bands': ['bareProb', 'vegProb', 'waterProb']}\n","\n","predictionMapid = predictionsImage.getMapId(predictionVis)\n","probabilityMapid = predictionsImage.getMapId(probabilityVis)\n","\n","map = folium.Map(location=[38., -122.5])\n","folium.TileLayer(\n","  tiles=EE_TILES.format(**predictionMapid),\n","  attr='Google Earth Engine',\n","  overlay=True,\n","  name='prediction',\n",").add_to(map)\n","folium.TileLayer(\n","  tiles=EE_TILES.format(**probabilityMapid),\n","  attr='Google Earth Engine',\n","  overlay=True,\n","  name='probability',\n",").add_to(map)\n","map.add_child(folium.LayerControl())\n","map"],"execution_count":0,"outputs":[]}]}